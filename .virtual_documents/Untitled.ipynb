# **Importing Dependencies**
# Import required libraries
import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')  # Download stopwords for text preprocessing
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)


# **Load the Dataset**
# Load the email dataset (spam/ham classification)
df = pd.read_csv('./spam.csv')

# Separate the input features (email text) and the target variable (spam/ham)
X = df['email_text']
y = df['spam']

# Create separate DataFrames for spam and ham emails
spam_df = df[y == 'spam']
ham_df = df[y == 'ham']

# Explore dataset statistics
num_emails = len(df)  # Total number of emails
num_spam = len(spam_df)  # Number of spam emails
num_ham = len(ham_df)  # Number of ham emails

# Print dataset information
print(f"Loaded {num_emails} emails from the dataset.")
print('Number of spam emails:', num_spam, " i.e. ", round(((num_spam / num_emails) * 100), 3), "%")
print('Number of ham emails:', num_ham, " i.e. ", round(((num_ham / num_emails) * 100), 3), "%")



# **Preprocessing the Text**
# Remove stopwords from email text using NLTK
stop_words = set(stopwords.words('english'))

# Update the email text after removing stopwords
X = X.apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))

# Update spam and ham DataFrames to reflect the cleaned email text
spam_df['email_text'] = X
ham_df['email_text'] = X
spam_df['email_text'] = spam_df['email_text'].astype(str)
ham_df['email_text'] = ham_df['email_text'].astype(str)



# **Vectorizing and Splitting Data**
# Convert email text into numerical format using CountVectorizer
vectorizer = CountVectorizer()
X_vect = vectorizer.fit_transform(X)

# Split dataset into training and testing sets
test_size =0.2
random_state = 10  # Fixed seed for reproducibility
X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=test_size, random_state=random_state)

# **Train the Multinomial Naive Bayes (MNB) Model**
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)



# save modeel 
import pickle
with open('./nb_trained_model.pkl', 'wb') as file:  
    pickle.dump(nb_model, file)


# save vectorizer 
import pickle
with open('./vectorizer.pkl', 'wb') as file:  
    pickle.dump(vectorizer, file)


# **Evaluate the Model**
# Predict labels for the testing set
y_pred = nb_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred) * 100
precision = precision_score(y_test, y_pred, pos_label='spam') * 100
recall = recall_score(y_test, y_pred, pos_label='spam') * 100
f1 = f1_score(y_test, y_pred, pos_label='spam') * 100

# Display results
print("\n------------------- Results -------------------")
print("Accuracy:  ", round(accuracy, 3), "%")
print("Precision: ", round(precision, 3), "%")
print("Recall:    ", round(recall, 3), "%")
print("F1 Score:  ", round(f1, 3), "%")
print("------------------------------------------------")




